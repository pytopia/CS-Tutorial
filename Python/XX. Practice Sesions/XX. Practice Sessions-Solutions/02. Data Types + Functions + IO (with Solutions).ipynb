{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"../../../images/banners/python-practice.png\" width=\"600\"/>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# <img src=\"../../../images/logos/python.png\" width=\"23\"/> 02. Practice Session \n", "\n", "> Covering Data Types, Functions, and IO."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Table of Contents\n\n\n* [Search Engine (TF-IDF)](#search_engine_(tf-idf))\n    * [Step #1: Read Files](#step_#1:_read_files)\n    * [Step #3: Extract Unique Words in all Documents](#step_#3:_extract_unique_words_in_all_documents)\n    * [Step #2: Extract Number of Words in each Document](#step_#2:_extract_number_of_words_in_each_document)\n    * [Step #3: Create `tf` (Term Frequency)](#step_#3:_create_`tf`_(term_frequency))\n\n---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a class=\"anchor\" id=\"search_engine_(tf-idf)\"></a>", "\n", "## Search Engine (TF-IDF)\n", "\n", "> **TF-IDF** stands for \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus.\n", "\n", "If i give you a sentence for example _\u201cThis building is so tall\u201d_. Its easy for us to understand the sentence as we know the semantics of the words and the sentence. But how will the computer understand this sentence? The computer can understand any data only in the form of numerical value. So, for this reason we vectorize all of the text so that the computer can understand the text better."]}, {"cell_type": "markdown", "metadata": {}, "source": ["By vectorizing the documents we can further perform multiple tasks such as finding the relevant documents, ranking, clustering and so on. This is the same thing that happens when you perform a google search. The web pages are called documents and the search text with which you search is called a query. google maintains a fixed representation for all of the documents. When you search with a query, google will find the relevance of the query with all of the documents, ranks them in the order of relevance and shows you the top k documents, all of this process is done using the vectorized form of query and documents. Although Googles algorithms are highly sophisticated and optimized, this is their underlying structure."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Terminology\n", "- **t**: term (word)\n", "- **d**: document (set of words)\n", "- **N**: count of corpus\n", "- **corpus**: the total document set"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a class=\"anchor\" id=\"step_#1:_read_files\"></a>", "\n", "### Step #1: Read Files\n", "\n", "> 1. Read the `data/files_path.txt` which contains all the documents you have to read.\n", "> 2. Read the files listed in `data/files_path.txt` and create a dictionary where keys are file names and values are file contents.\n", "\n", "```python\n", "docs = {\n", "    \"file_1\": \"content_1\",\n", "    \"file_2\": \"content_2\",\n", "    ...\n", "} \n", "```"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["docs = {}"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["with open(\"./data/files_path.txt\") as f:\n", "    files = [f.strip() for f in f.readlines()]"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["for file_path in files:\n", "    with open(file_path) as f:\n", "        name = file_path.split(\"/\")[-1].split(\".\")[0]\n", "        if name in docs:\n", "            docs[name] += f.read()\n", "        else:\n", "            docs[name] = f.read()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a class=\"anchor\" id=\"step_#3:_extract_unique_words_in_all_documents\"></a>", "\n", "### Step #3: Extract Unique Words in all Documents\n", "\n", "> Create a set of all words (`vocab`) and print the number of unique words."]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["vocab = set()\n", "for name, content in docs.items():\n", "    vocab.update(content.split())"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["8937\n"]}], "source": ["print(len(vocab))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a class=\"anchor\" id=\"step_#2:_extract_number_of_words_in_each_document\"></a>", "\n", "### Step #2: Extract Number of Words in each Document\n", "\n", "> 1. Extract words in each document by creating a dictionary named `tf_dict` where keys are document names and values are another dictionary.\n", "> 2. In the nested dictionary, keys are words and values are the corresponding word frequency."]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["tf_dict = {}\n", "for name, content in docs.items():\n", "    \n", "    word_dict = {}\n", "    for w in content.split():\n", "        if w in word_dict:\n", "            word_dict[w] += 1\n", "        else:\n", "            word_dict[w] = 1\n", "            \n", "    tf_dict[name] = word_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a class=\"anchor\" id=\"step_#3:_create_`tf`_(term_frequency)\"></a>", "\n", "### Step #3: Create `tf` (Term Frequency)\n", "\n", "> 1. Create a dictionary where words are keys and values are a list.\n", "> 2. Values are a list of corresponding documents frequencies.\n", "\n", "```python\n", "tf = {\n", "    word_1: [freq_doc_1, freq_doc_2, freq_doc_3, ..., freq_doc_n],\n", "    word_2: [freq_doc_1, freq_doc_2, freq_doc_3, ..., freq_doc_n],\n", "    word_3: [freq_doc_1, freq_doc_2, freq_doc_3, ..., freq_doc_n],\n", "    ...\n", "    word_n: [freq_doc_1, freq_doc_2, freq_doc_3, ..., freq_doc_n],\n", "}\n", "```\n", "\n", "| |doc_1|doc_2|...|doc_n|\n", "|--|--|--|--|--|\n", "|word_1|10|4|...|14|\n", "|word_2|8|11|...|4|\n", "|word_3|3|5|...|1|"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["tf = {}"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8937/8937 [00:00<00:00, 106531.50it/s]\n"]}], "source": ["for w in tqdm(vocab):\n", "    vector = []\n", "    for name, word_freq in tf_dict.items():\n", "        vector.append(word_freq.get(w, 0))\n", "        \n", "    tf[w] = vector"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"data": {"text/plain": ["28"]}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": ["import numpy as np\n", "np.argmax([i*j for i, j in zip(tf[\"\u0645\u062d\u0633\u0646\"], tf[\"\u0646\u0642\u0634\"])])"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"data": {"text/plain": ["'Mohsen'"]}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": ["list(tf_dict.keys())[28]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.10"}}, "nbformat": 4, "nbformat_minor": 4}